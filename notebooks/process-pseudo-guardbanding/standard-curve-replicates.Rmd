---
title: "Standard Curve Replicates"
author: "Dilsher Singh Dhillon"
date: "`r format(Sys.time(),'%d %B,%Y')`"
output:
  html_document:
    toc: true
    toc_float : true  
---
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #948DFF;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
## Here are the libraries to install  
library(tidyverse)  
#library(nls.multstart)
#library(broom)
#library(minpack.lm)
freenome_colors <- c('#948DFF', '#1DE3FE', '#BBD532', '#FF9D42',  '#FC2E7B', 
                   '#FECDD1')
#library(furrr)
```


## Data Import 
```{r eval=FALSE, include=FALSE}
library(googleCloudStorageR)
googleAuthR::gar_gce_auth()
gcs_get_object("gs://inhouse-xmap-data/user_data_frames/betalot_experiments-2022-02-21.csv", saveToDisk="/home/ddhillon/projects/beta-av-testing/data/raw/beta-lot-stds-controls-2022-02-21.csv", 
               overwrite = TRUE)
```


```{r}
## import data till 22nd february and pre-process like we usually do 
raw <-
  read_csv(here::here("data", "raw", "beta-lot-stds-controls-2022-02-21.csv")) %>%
  filter(assay %in% c("WFDC2", "CEA", "IL-8", "IL-1 R2", "TNC", "MUC-16", "FLT3L")) %>%
  mutate(batch = str_remove(file_name, "_[0-9]+.csv")) %>%
  filter(!grepl("20211028_Reproducibilit_Panel2_Plate3", file_name)) %>% ## this was a bad batch
  mutate(experiment_date = str_extract(batch, "[0-9]+_")) %>%
  mutate(experiment_date = str_extract(batch, "[0-9]+")) %>%
  mutate(experiment_date = lubridate::as_date(experiment_date, format = "%Y%d%m")) %>%
  mutate(
    month = lubridate::month(experiment_date),
    year = lubridate::year(experiment_date)
  ) %>%
  mutate(bead_fail = ifelse(bead_count < 35, 1, 0)) 

```



## Functions we need 


```{r}
mod_z_calc <- function(x_vec) {
  x_vec_median <- median(x_vec, na.rm = TRUE) 
  x_vec_mad <- mad(x_vec, na.rm = TRUE)
  
  mod_z <- 0.6745*(x_vec - x_vec_median)/x_vec_mad
  return(mod_z)
  
}
```

Pick data we want to work with 

1. No bead failures in any of the standards 
2. Take experiments that only have 4 standards in a batch (how our process works) 
3. Exclude standards which have salts mixed in standards (this isn't reflective of our process)   

```{r}
## find all the batches that have 4 replicates for each standard (quads) 
clean_data <- raw %>% 
  filter(bead_fail == 0) %>% ## no bead failures 
  filter(sample_type == "standard") %>% 
  filter(!grepl("Salt", xponent_id)) %>% 
  count(batch, xponent_id, assay) %>% 
  filter(n == 4) %>% 
  distinct(batch) %>% 
  inner_join(raw) %>% 
  ungroup()

clean_std_data <- clean_data %>% 
  filter(sample_type == "standard")
```

## Modeling   



### Model data 

```{r}
mod_data <- clean_std_data %>% 
  group_by(xponent_id, assay, batch) %>% 
  mutate(rep_num = dplyr::row_number(xponent_id)) %>% 
  select(xponent_id, assay, rep_num, median_mfi, 
         standard_expected_concentration, 
         calc_conc, pct_recovery, batch, net_mfi) %>% 
  ungroup()
```

```{r eval=FALSE}
## we do this beacuse I needed Adam's help with the fitting the models in python so he could access the data 
write_csv(mod_data, here::here("data", "processed", "std-curve-replicate-analysis", 
                               "original_mod_data.csv"))
```

### Simulate Missing Data 

```{r}
## these are all the ways we can get 2 replicates 
all_combs <- combn(4, 2) %>% 
  as_tibble(.name_repair = "unique") %>% 
  janitor::clean_names() %>% 
  tidyr::pivot_longer(., cols =c(x1:x6), names_to = "sam", values_to = "rep_num")
```


```{r}
## we create a function that created a dataset with 2 replicates missing from a given standard 
create_missing_data <-
  function(comb = "NULL",
           standard = "NULL",
           data = NULL) {
    rep_comb <- all_combs %>%
      dplyr::filter(sam == comb) %>%
      select(rep_num)
    
    
    tmp_data <- data %>%
      dplyr::filter(xponent_id != standard)
    
    data %>%
      dplyr::filter(xponent_id == standard) %>%
      inner_join(., rep_comb, by = "rep_num") %>%
      bind_rows(tmp_data)
  }
```

Create sets of missing data 

```{r message=FALSE}
combs <- c("x1", "x2", "x3", "x4", "x5", "x6") 
stds <- c("Standard1", "Standard2", "Standard3", "Standard4", "Standard5", 
          "Standard6")

grid <- tidyr::crossing(combs = combs, stds = stds) %>% 
  mutate(iter = seq(1, nrow(.), by = 1))

missing_data <- grid %>% 
  rowwise(iter) %>% 
  mutate(data = list(create_missing_data(combs, stds, data = mod_data))) %>% 
  unnest(cols = data)
```


```{r eval=FALSE}
## we export this out as well 
write_csv(missing_data, here::here("data", "processed", 
                                   "std-curve-replicate-analysis", "missing-sim-data.csv"))
```




### Scipy implementation   

```{r}
library(reticulate)
```

```{r}
Sys.which("python")

use_python(python = "/home/ddhillon/.virtualenvs/pate/bin/python")
```

```{python}
import numpy as np
import pandas as pd

from scipy.optimize.minpack import curve_fit
from joblib import Parallel, delayed
```



```{python}
def curve_fit_coeffs(
    exp_conc_protein: pd.Series, net_mfi_protein: pd.Series) -> np.ndarray:
    """Find a curve fit using the standard concentrations and MFIs from the panel xPonent file.
    Args:
        exp_conc_protein: expected concentrations of the standards
        net_mfi_protein: net MFIs of the standards
    Returns: numpy array of the curve fit coefficients
    """
    min_mfi = net_mfi_protein.min()
    max_mfi = net_mfi_protein.max()
    p0 = [min_mfi, 1, exp_conc_protein.median(), max_mfi, 1]
    xdata = list(exp_conc_protein)
    ydata = list(net_mfi_protein)
    coeffs, _ = curve_fit(
        f=logistic5p,
        xdata=xdata,
        ydata=ydata,
        p0=p0,
        sigma=ydata,
        method="lm",
        maxfev=int(1e5),
        absolute_sigma=True,
    )
    return coeffs

```

```{python}
def logistic5p(x: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """5-parameter logistic equation"""
    return D + (A - D) / ((1 + (x / C) ** B) ** F)
```


```{python}
def inv_logistic5p(y: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """Inverse of the 5-parameter logistic equation"""
    return C * (((((A - D) / (y - D)) ** (1.0 / F)) - 1.0) ** (1.0 / B))
```

```{python}
def parallel_runner(sub_df):
    sub_df = sub_df.loc[~sub_df.isna()["net_mfi"].values]
    estimated_coefficients = curve_fit_coeffs(sub_df["standard_expected_concentration"], sub_df["net_mfi"])
    inferred_concentrations = np.array([inv_logistic5p(net_mfi, *estimated_coefficients) for net_mfi in sub_df["net_mfi"]])
    sub_df["inferred_concentration"] = inferred_concentrations
    return sub_df

```


```{python}
!pwd
```


```{python}
n_cpu = 8
PARALLEL = True
```


```{python}
missing_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/missing-sim-data.csv')
original_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/original_mod_data.csv')
```

### Fit Clean Data Model  

Fit models to original data   

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in original_df.groupby(["batch", "assay"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(original_df.groupby(["batch", "assay"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

original_result_df = pd.concat(result_df)
```




Confirm with the xPonent concentrations 

For S3-S6, the concordance in recoveries very high, but there is a fair bit of discordance and noise in the first 2 standards for some proteins. For many of our proteins, this is already above our ULOQ. 

```{r}
py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery)) + 
  geom_boxplot() + 
  theme_bw() + 
  facet_grid(cols = vars(assay)) + 
  geom_hline(yintercept = 100, color = freenome_colors[1]) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery_scipy)) + 
  geom_boxplot() + 
  theme_bw() + 
  facet_grid(cols = vars(assay)) + 
  geom_hline(yintercept = 100, color = freenome_colors[1]) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



## some edge cases in CEA 
py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>%
  filter(pct_recovery_scipy > 200) %>% 
  filter(assay == "CEA")


py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  filter(pct_recovery_scipy < 200) %>% 
  ggplot(.,aes(pct_recovery, pct_recovery_scipy)) +
  geom_point() + 
  facet_grid(rows = vars(xponent_id), 
             cols = vars(assay)) + 
  theme_bw()

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  filter(pct_recovery_scipy > 200) %>% 
  ggplot(.,aes(pct_recovery, pct_recovery_scipy)) +
  geom_point() + 
  facet_grid(rows = vars(xponent_id), 
             cols = vars(assay)) + 
  theme_bw()

```

### Fit Missing Data Models  

Fit models to the missing data we created 

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in missing_df.groupby(["batch", "assay", "combs", "stds"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(missing_df.groupby(["batch", "assay", "combs", "stds"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

missing_result_df = pd.concat(result_df)
```


```{r}
missing_recov <- py$missing_result_df %>% 
  mutate(pct_recovery_missing = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  group_by(assay, batch, xponent_id, combs, stds) %>% 
  summarise(pct_recovery_missing = mean(pct_recovery_missing)) %>% 
  ungroup()

original_recov <- py$original_result_df %>% 
  mutate(pct_recovery_original = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  group_by(assay, batch, xponent_id) %>% 
  summarise(pct_recovery_original = mean(pct_recovery_original)) %>% 
  ungroup()
```




```{r}
comb_recov <- missing_recov %>% 
  select(c(combs, stds, assay, batch, xponent_id, pct_recovery_missing)) %>% 
  left_join(., original_recov, by = c("assay", "batch", "xponent_id"))
```

```{r}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

```


## Natural Tolerance Limits  

We want to compare the results of fitting a curve without a few replicates against what would happen if we simply ran the batch again. If the noise associated with less replicates <= noise associated with running a new batch, we can simply go ahead and fit the curve again. But if this noise is > noise associated with running a new batch, we should then choose to run a new batch.  



```{r}
## here we don't average over the quads 
natural_limits_V1 <- py$original_result_df %>% 
  mutate(pct_recovery_original = 100*(inferred_concentration)/standard_expected_concentration) %>%
  filter(pct_recovery_original < 1000) %>% 
  group_by(xponent_id, assay) %>%
  nest() %>% 
  mutate(tol_df = map(data, ~ tolerance::normtol.int(.x$pct_recovery_original, 
                                                     method = "EXACT", side = 2, m = 500, 
                           log.norm = FALSE, P = 0.95))) %>% 
  unnest(cols = tol_df) %>% 
  janitor::clean_names() %>%
  mutate(lower_diff = (x_bar - x2_sided_lower), 
         upper_diff = (x2_sided_upper - x_bar)) %>% 
  select(assay, xponent_id, lower_diff, upper_diff) %>% 
  ungroup()
  #group_by(assay, batch, xponent_id) %>% 
  #summarise(pct_recovery_original = mean(pct_recovery_original)) %>% 
  #ungroup()

## here we average over the quads
natural_limits_V2 <- original_recov %>% 
  filter(pct_recovery_original < 1000) %>% ## there are a few edge cases which we ignore  
  group_by(xponent_id, assay) %>%
  nest() %>% 
  mutate(tol_df = map(data, ~ tolerance::normtol.int(.x$pct_recovery_original, 
                                                     method = "EXACT", side = 2, m = 500, 
                           log.norm = FALSE, P = 0.95))) %>% 
  unnest(cols = tol_df) %>% 
  janitor::clean_names() %>%
  mutate(lower_diff = (x_bar - x2_sided_lower), 
         upper_diff = (x2_sided_upper - x_bar)) %>% 
  select(assay, xponent_id, lower_diff, upper_diff) %>% 
  ungroup()

natural_limits_V1
natural_limits_V2
```



**How does the missing replicate models compare against natural variation?** 

```{r fig.height=10}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  #geom_errorbar(aes(ymin = lower_error, ymax = upper_error)) + 
  geom_errorbar(aes(ymin = -lower_diff, ymax = upper_diff), color = freenome_colors[1]) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) 


comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  #geom_errorbar(aes(ymin = lower_error, ymax = upper_error)) + 
  geom_errorbar(aes(ymin = -lower_diff, ymax = upper_diff), color = freenome_colors[1], 
                alpha = 0.5) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) 
```

















