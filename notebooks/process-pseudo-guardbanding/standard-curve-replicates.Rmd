---
title: "Standard Curve Replicates"
author: "Dilsher Singh Dhillon"
date: "`r format(Sys.time(),'%d %B,%Y')`"
output:
  html_document:
    toc: true
    toc_float : true  
---
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #948DFF;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
## Here are the libraries to install  
library(tidyverse)  
library(nls.multstart)
library(broom)
#library(minpack.lm)
freenome_colors <- c('#948DFF', '#1DE3FE', '#BBD532', '#FF9D42',  '#FC2E7B', 
                   '#FECDD1')
#library(furrr)
```


## Data Import 

Ran a PATE command that then saved the file in the location below in GCS. Use `googleCloudStorageR` to import the data in JH and read from there 


```
pate-rnd xmap-collector --include_file_str 20220119_WC_P1vP6 \
  --include_file_str  20220120_ShakeSpd_1000rpm_Panel1_Plate1 \
  --include_file_str  20220120_ShakeSpd_1200rpm_Panel1_Plate2 \
  --include_file_str  20220120_ShakeSpd_1500rpm_Panel1_Plate3 \
  --include_file_str  20220120_ShakeSpd_1000rpm_Panel2_Plate1 \
  --include_file_str  20220120_ShakeSpd_1200rpm_Panel2_Plate2 \
  --include_file_str  20220120_ShakeSpd_1500rpm_Panel2_Plate3 \
  --include_file_str  20220120_ShakeSpd_1200rpm_Panel2_Plate2 \
  --include_file_str  20220120_ShakeSpd_1500rpm_Panel2_Plate2 \
  --include_file_str  Beta \
  --include_file_str  Interf \
  --include_file_str  PltStd \
  --include_file_str  Endogenous \
  --include_file_str  Exogenous \
  --include_file_str  Platinum \
  --include_file_str  Reproducibility \
  --include_file_str  Donor_Screening \
  --include_file_str  PlatinumStd \
  --include_file_str  HamiltonEquiv\
  --include_file_str  ShakingCompare \
  --include_file_str  20211221_DLS_Matrix_Int \
  --include_file_str  20220124_BufferAdditives \
  --include_file_str  20220208_CRC_EDTA \
  --include_file_str  Matrix_Buffer_TritonX \
  --include_file_str  20220215_WCFunctional \
  --include_file_str  20220217_Linearity_Streck \
  --include_sample_str Standard --include_sample_str Buffer --include_sample_str Background --include_sample_str QC --include_sample_str RC  --ignore_case_sample \
  --out_file_name betalot_experiments-2022-02-21.csv
```



```{r eval=FALSE, include=FALSE}
library(googleCloudStorageR)
googleAuthR::gar_gce_auth()
gcs_get_object("gs://inhouse-xmap-data/user_data_frames/betalot_experiments-2022-02-21.csv", saveToDisk="/home/ddhillon/projects/beta-av-testing/data/raw/beta-lot-stds-controls-2022-02-21.csv", 
               overwrite = TRUE)
```


```{r}
## import data till 22nd february and pre-process like we usually do 
raw <-
  read_csv(here::here("data", "raw", "beta-lot-stds-controls-2022-02-21.csv")) %>%
  filter(assay %in% c("WFDC2", "CEA", "IL-8", "IL-1 R2", "TNC", "MUC-16", "FLT3L")) %>%
  mutate(batch = str_remove(file_name, "_[0-9]+.csv")) %>%
  filter(!grepl("20211028_Reproducibilit_Panel2_Plate3", file_name)) %>% ## this was a bad batch
  mutate(experiment_date = str_extract(batch, "[0-9]+_")) %>%
  mutate(experiment_date = str_extract(batch, "[0-9]+")) %>%
  mutate(experiment_date = lubridate::as_date(experiment_date, format = "%Y%d%m")) %>%
  mutate(
    month = lubridate::month(experiment_date),
    year = lubridate::year(experiment_date)
  ) %>%
  mutate(bead_fail = ifelse(bead_count < 35, 1, 0)) 

```



**Functions we need**   

```{r}
mod_z_calc <- function(x_vec) {
  x_vec_median <- median(x_vec, na.rm = TRUE) 
  x_vec_mad <- mad(x_vec, na.rm = TRUE)
  
  mod_z <- 0.6745*(x_vec - x_vec_median)/x_vec_mad
  return(mod_z)
  
}
```

Pick data we want to work with 

1. No bead failures in any of the standards 
2. Take experiments that only have 4 quads for each standard/calibrator in a batch (how our process works) 
3. Exclude standards which have salts mixed in standards (this isn't reflective of our process)   
    *There were a couple experiments where the dev team was troubleshooting issues other than bead count using salts in standards* 

```{r}
## find all the batches that have 4 replicates for each standard (quads) 
clean_data <- raw %>% 
  filter(bead_fail == 0) %>% ## no bead failures 
  filter(sample_type == "standard") %>% 
  filter(!grepl("Salt", xponent_id)) %>% 
  count(batch, xponent_id, assay) %>% 
  filter(n == 4) %>% 
  distinct(batch) %>% 
  inner_join(raw) %>% 
  ungroup()

clean_std_data <- clean_data %>% 
  filter(sample_type == "standard")
```

## Modeling   



### Model data 

```{r}
mod_data <- clean_std_data %>% 
  group_by(xponent_id, assay, batch) %>% 
  mutate(rep_num = dplyr::row_number(xponent_id)) %>% 
  select(xponent_id, assay, rep_num, median_mfi, 
         standard_expected_concentration, 
         calc_conc, pct_recovery, batch, net_mfi) %>% 
  ungroup() 

mod_data <- mod_data %>% 
  filter(net_mfi > 0)
```

```{r eval=FALSE}
## we do this beacuse I needed Adam's help with the fitting the models in python so he could access the data 
write_csv(mod_data, here::here("data", "processed", "std-curve-replicate-analysis", 
                               "original_mod_data_positive_net_mfi.csv"))
```

### Simulate Missing Data  

To simulate missing data - 

1. We create missing combinations of 4 replicates - there are ways 2 replicates can end up from a set of 4 replicates.  
[1,2] , [1,3], [1,4] , [2,3], [2,4], [3,4]  
2. Pick a standard for each protein, pick one set of duplicates for that standard from the above 6 combinations. 
3. Fit a curve 

Repeat this for every standard 

```{r}
## these are all the ways we can get 2 replicates
all_combs <- combn(4, 2) %>%
  as_tibble(.name_repair = "unique") %>%
  janitor::clean_names() %>%
  tidyr::pivot_longer(.,
                      cols = c(x1:x6),
                      names_to = "sam",
                      values_to = "rep_num")
```


```{r}
## we create a function that created a dataset with 2 replicates missing from a given standard 
create_missing_data <-
  function(comb = "NULL",
           standard = "NULL",
           data = NULL) {
    rep_comb <- all_combs %>%
      dplyr::filter(sam == comb) %>%
      select(rep_num)
    
    
    tmp_data <- data %>%
      dplyr::filter(xponent_id != standard)
    
    data %>%
      dplyr::filter(xponent_id == standard) %>%
      inner_join(., rep_comb, by = "rep_num") %>%
      bind_rows(tmp_data)
  }
```

Create sets of missing data 

```{r message=FALSE}
combs <- c("x1", "x2", "x3", "x4", "x5", "x6")
stds <-
  c("Standard1",
    "Standard2",
    "Standard3",
    "Standard4",
    "Standard5",
    "Standard6")

## for each standard and 
grid <- tidyr::crossing(combs = combs, stds = stds) %>%
  mutate(iter = seq(1, nrow(.), by = 1))

missing_data <- grid %>%
  rowwise(iter) %>%
  mutate(data = list(create_missing_data(combs, stds, data = mod_data))) %>%
  unnest(cols = data)
```


```{r eval=FALSE}
## we export this out as well 
write_csv(missing_data, here::here("data", "processed", 
                                   "std-curve-replicate-analysis", "missing-sim-data_positive_net_mfi.csv"))
```




### Scipy implementation   

```{r}
library(reticulate)
```

```{r}
Sys.which("python")

use_python(python = "/home/ddhillon/.virtualenvs/pate/bin/python")
```

```{python}
import numpy as np
import pandas as pd

from scipy.optimize.minpack import curve_fit
from joblib import Parallel, delayed
```



```{python}
def curve_fit_coeffs(
    exp_conc_protein: pd.Series, net_mfi_protein: pd.Series) -> np.ndarray:
    """Find a curve fit using the standard concentrations and MFIs from the panel xPonent file.
    Args:
        exp_conc_protein: expected concentrations of the standards
        net_mfi_protein: net MFIs of the standards
    Returns: numpy array of the curve fit coefficients
    """
    min_mfi = net_mfi_protein.min()
    max_mfi = net_mfi_protein.max()
    p0 = [min_mfi, 1, exp_conc_protein.median(), max_mfi, 1]
    xdata = list(exp_conc_protein)
    ydata = list(net_mfi_protein)
    coeffs, _ = curve_fit(
        f=logistic5p,
        xdata=xdata,
        ydata=ydata,
        p0=p0,
        sigma=ydata,
        method="lm",
        maxfev=int(1e5),
        absolute_sigma=True,
    )
    return coeffs

```

```{python}
def logistic5p(x: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """5-parameter logistic equation"""
    return D + (A - D) / ((1 + (x / C) ** B) ** F)
```


```{python}
def inv_logistic5p(y: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """Inverse of the 5-parameter logistic equation"""
    return C * (((((A - D) / (y - D)) ** (1.0 / F)) - 1.0) ** (1.0 / B))
```

```{python}
def parallel_runner(sub_df):
    sub_df = sub_df.loc[~sub_df.isna()["net_mfi"].values]
    estimated_coefficients = curve_fit_coeffs(sub_df["standard_expected_concentration"], sub_df["net_mfi"])
    inferred_concentrations = np.array([inv_logistic5p(net_mfi, *estimated_coefficients) for net_mfi in sub_df["net_mfi"]])
    sub_df["inferred_concentration"] = inferred_concentrations
    return sub_df

```


```{python}
!pwd
```


```{python}
n_cpu = 8
PARALLEL = True
```


```{python}
missing_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/missing-sim-data.csv')
original_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/original_mod_data.csv')
```

### Fit Clean Data Model  

Fit models to original data   

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in original_df.groupby(["batch", "assay"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(original_df.groupby(["batch", "assay"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

original_result_df = pd.concat(result_df)
```




Confirm with the xPonent concentrations 

For S3-S6, the concordance in recoveries very high, but there is a fair bit of discordance and noise in the first 2 standards for some proteins. For many of our proteins, this is already above our ULOQ. 

```{r}
py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery)) + 
  geom_boxplot() + 
  theme_bw() + 
  facet_grid(cols = vars(assay)) + 
  geom_hline(yintercept = 100, color = freenome_colors[1]) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery_scipy)) + 
  geom_boxplot() + 
  theme_bw() + 
  facet_grid(cols = vars(assay)) + 
  geom_hline(yintercept = 100, color = freenome_colors[1]) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



## some edge cases in CEA 
py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>%
  filter(pct_recovery_scipy > 200) %>% 
  filter(assay == "CEA")


py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  filter(pct_recovery_scipy < 200) %>% 
  ggplot(.,aes(pct_recovery, pct_recovery_scipy)) +
  geom_point() + 
  facet_grid(rows = vars(xponent_id), 
             cols = vars(assay)) + 
  theme_bw()

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  filter(pct_recovery_scipy > 200) %>% 
  ggplot(.,aes(pct_recovery, pct_recovery_scipy)) +
  geom_point() + 
  facet_grid(rows = vars(xponent_id), 
             cols = vars(assay)) + 
  theme_bw()

```

### Fit Missing Data Models  

Fit models to the missing data we created 

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in missing_df.groupby(["batch", "assay", "combs", "stds"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(missing_df.groupby(["batch", "assay", "combs", "stds"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

missing_result_df = pd.concat(result_df)
```


```{r}
## average recovery over all batches, assay, xponent_id and the standard that was deleted 
missing_recov <- py$missing_result_df %>%
  mutate(pct_recovery_missing = 100 * (inferred_concentration) / standard_expected_concentration) %>%
  group_by(assay, combs, batch, xponent_id, stds) %>%
  summarise(pct_recovery_missing = mean(pct_recovery_missing, na.rm = TRUE)) %>%
  ungroup()

original_recov <- py$original_result_df %>%
  mutate(
    pct_recovery_original = 100 * (inferred_concentration) / standard_expected_concentration
  ) %>%
  group_by(assay, batch, xponent_id) %>%
  summarise(pct_recovery_original = mean(pct_recovery_original, na.rm = TRUE)) %>%
  ungroup()
```




```{r}
comb_recov <- missing_recov %>% 
  select(c(stds, combs, assay, batch, xponent_id, pct_recovery_missing)) %>% 
  left_join(., original_recov, by = c("assay", "batch", "xponent_id")) %>% 
  ungroup()
```

```{r}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

```


## Natural Tolerance Limits  

We want to compare the results of fitting a curve without a few replicates against what would happen if we simply ran the batch again. If the noise associated with less replicates <= noise associated with running a new batch, we can simply go ahead and fit the curve again. But if this noise is > noise associated with running a new batch, we should then choose to run a new batch.  



```{r}
## here we don't average over the quads 
natural_limits_V1 <- py$original_result_df %>% 
  mutate(pct_recovery_original = 100*(inferred_concentration)/standard_expected_concentration) %>%
  filter(pct_recovery_original < 1000) %>% 
  group_by(xponent_id, assay) %>%
  nest() %>% 
  mutate(tol_df = map(data, ~ tolerance::normtol.int(.x$pct_recovery_original, 
                                                     method = "EXACT", side = 2, m = 500, 
                           log.norm = FALSE, P = 0.95))) %>% 
  unnest(cols = tol_df) %>% 
  janitor::clean_names() %>%
  mutate(lower_diff = (x_bar - x2_sided_lower), 
         upper_diff = (x2_sided_upper - x_bar)) %>% 
  select(assay, xponent_id, lower_diff, upper_diff) %>% 
  ungroup()
  #group_by(assay, batch, xponent_id) %>% 
  #summarise(pct_recovery_original = mean(pct_recovery_original)) %>% 
  #ungroup()

## here we average over the quads
natural_limits_V2 <- original_recov %>% 
  filter(pct_recovery_original < 1000) %>% ## there are a few edge cases which we ignore  
  group_by(xponent_id, assay) %>%
  nest() %>% 
  mutate(tol_df = map(data, ~ tolerance::normtol.int(.x$pct_recovery_original, 
                                                     method = "EXACT", side = 2, m = 500, 
                           log.norm = FALSE, P = 0.95))) %>% 
  unnest(cols = tol_df) %>% 
  janitor::clean_names() %>%
  mutate(lower_diff = (x_bar - x2_sided_lower), 
         upper_diff = (x2_sided_upper - x_bar)) %>% 
  select(assay, xponent_id, lower_diff, upper_diff) %>% 
  ungroup()

natural_limits_V1
natural_limits_V2

natural_limits_V3 <- mod_data %>% 
  group_by(assay, xponent_id, batch) %>% 
  summarise(pct_recovery = mean(pct_recovery, na.rm = TRUE)) %>% 
  ungroup() %>% 
  filter(pct_recovery < 1000) %>% ## there are a few edge cases which we ignore  
  group_by(xponent_id, assay) %>%
  nest() %>% 
  mutate(tol_df = map(data, ~ tolerance::normtol.int(.x$pct_recovery, 
                                                     method = "EXACT", side = 2, m = 500, 
                           log.norm = FALSE, P = 0.95))) %>% 
  unnest(cols = tol_df) %>% 
  janitor::clean_names() %>%
  mutate(lower_diff = (x_bar - x2_sided_lower), 
         upper_diff = (x2_sided_upper - x_bar)) %>% 
  select(assay, xponent_id, lower_diff, upper_diff) %>% 
  ungroup()

natural_limits_V2
natural_limits_V3
```



**How does the missing replicate models compare against natural variation?** 

We expect 95 % of the experiments we conducted with simulated data to be within the 95 % tolerance intervals.  

```{r}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  mutate(outside_interval = ifelse(abs(diff) > lower_diff, "out", "in")) %>% 
  count(assay, xponent_id, stds, outside_interval) %>% 
  group_by(assay, xponent_id, stds) %>%
  mutate(freq = 100*(n / sum(n))) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  ungroup() %>% 
  filter(outside_interval == "out") %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, freq, color = perturbed)) + 
  geom_point(size = 2) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) +
  geom_hline(yintercept = 5, linetype="dotted") + 
  labs(title = "% of time the recoveries from missing replicates outside tolerance limits", 
       subtitle = "95 % Tolerance Limits can expect 5 % to be outside by chance")

comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  mutate(outside_interval = ifelse(abs(diff) > lower_diff, "out", "in")) %>% 
  count(assay, xponent_id, stds, outside_interval) %>% 
  group_by(assay, xponent_id, stds) %>%
  mutate(freq = 100*(n / sum(n))) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  ungroup() %>% 
  filter(outside_interval == "out") %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, freq, color = perturbed)) + 
  geom_point(size = 2) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) +
  geom_hline(yintercept = 5, linetype="dotted") + 
  labs(title = "% of time the recoveries from missing replicates outside tolerance limits", 
       subtitle = "95 % Tolerance Limits can expect 5 % to be outside by chance")
```


```{r}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  #geom_errorbar(aes(ymin = lower_error, ymax = upper_error)) + 
  geom_errorbar(aes(ymin = -lower_diff, ymax = upper_diff), color = freenome_colors[1]) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) + 
  labs(title = "Purple Bars are the 95 % Tolerance Limts")


comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  left_join(., natural_limits_V2, by = c("assay", "xponent_id")) %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  #geom_errorbar(aes(ymin = lower_error, ymax = upper_error)) + 
  geom_errorbar(aes(ymin = -lower_diff, ymax = upper_diff), color = freenome_colors[1], 
                alpha = 0.5) + 
  facet_grid(rows = vars(assay), cols = vars(stds)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6)) + 
  scale_color_manual(values = c(freenome_colors[2], freenome_colors[3])) + 
  labs(title = "Purple Bars are the 95 % Tolerance Limts")
```























## R implementation 

```{r}
logistic_5pl <- function(d, a, mfi, c, b, g) {
  return(d + (a-d)/(1 + (mfi/c)^b)^g)
}

fit_5pl <- function(df = NULL){
  min_mfi <- df %>% pull(net_mfi) %>% min()
  max_mfi <- df %>% pull(net_mfi) %>% max() 
  exp_conc_protein <- df %>% pull(standard_expected_concentration) %>% median()
  p0 = c(min_mfi, 1, exp_conc_protein, max_mfi, 1)
  nls.multstart::nls_multstart(standard_expected_concentration ~ 
                                          logistic_5pl(d, a, mfi = net_mfi, 
                                                       c, b, g), 
                                        data = df, 
                                        iter = 10000, 
                                        modelweights = 1/net_mfi,
                                        start_lower = c(a = p0[1], b = p0[2], c = p0[3], d = p0[4], 
                                                        g = p0[5]), 
                                        start_upper = c(a = p0[1] + rnorm(1, 10, 1), b = p0[2] + + rnorm(1, 10, 1), 
                                                        c = p0[3] + + rnorm(1, 10, 1), d = p0[4] + + rnorm(1, 10, 1), 
                                                        g = p0[5] + + rnorm(1, 10, 1)), 
                                        supp_errors = "Y", 
                                        control = nls.control(maxiter = 10000, 
                                                              minFactor=1e-7, 
                                                              tol=1e-5, 
                                                              printEval=F, 
                                                              warnOnly=F))

}
weighted_5pl <- function(resp, d, a, net_mfi, c, b, g) {
  resp <- d + (a-d)/(1 + (net_mfi/c)^b)^g
}
fit_nls <- function(df = NULL){
  min_mfi <- df %>% pull(net_mfi) %>% min()
  max_mfi <- df %>% pull(net_mfi) %>% max() 
  exp_conc_protein <- df %>% pull(standard_expected_concentration) %>% median()
  p0 = c(min_mfi, 1, exp_conc_protein, max_mfi, 1)
  
  nls(~ weighted_5pl(resp = standard_expected_concentration, d, a, net_mfi, c, b, g), 
      weights = 1/net_mfi, 
      start = list(a = p0[1], b = p0[2], c = p0[3], d = p0[4], 
                                                        g = p0[5]), 
      data = df)
}
```

```{r}
original_mods_r <- mod_data %>% 
  group_by(batch, assay) %>% 
  nest() %>% 
  ungroup() %>% 
  slice(1) %>% 
  mutate(orig_model = map(data, ~ fit_5pl(df = .x))) 

original_mods_r <- mod_data %>% 
  group_by(batch, assay) %>% 
  nest() %>% 
  ungroup() %>% 
  slice(1) %>% 
  mutate(orig_model = map(data, ~ fit_nls(df = .x))) 

```


**fit original mod** 
```{r}
original_mods_r <- mod_data %>% 
  group_by(batch, assay) %>% 
  nest() %>% 
  ungroup() %>% 
  mutate(orig_model = map(data, ~ fit_5pl(df = .x))) 

original_recov_r <- original_mods_r %>%
  ungroup() %>%
  mutate(pred_frame = map2(orig_model, data, ~ augment(x = .x, newdata = .y))) %>%
  select(-c(data, orig_model)) %>%
  unnest(cols = pred_frame) %>%
  mutate(pct_recovery_r = 100 * .fitted / standard_expected_concentration) %>% 
  select(assay, batch, xponent_id, pct_recovery_r) %>% 
  group_by(assay, batch, xponent_id) %>% 
  summarise(pct_recovery_original = mean(pct_recovery_r)) %>% 
  ungroup()
```











