---
title: "Standard Curve Replicates"
author: "Dilsher Singh Dhillon"
date: "`r format(Sys.time(),'%d %B,%Y')`"
output:
  html_document:
    toc: true
    toc_float : true  
---
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #948DFF;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
## Here are the libraries to install  
library(tidyverse)  
#library(nls.multstart)
#library(broom)
#library(minpack.lm)
freenome_colors <- c('#948DFF', '#1DE3FE', '#BBD532', '#FF9D42',  '#FC2E7B', 
                   '#FECDD1')
#library(furrr)
```


## Data Import 
```{r eval=FALSE, include=FALSE}
library(googleCloudStorageR)
googleAuthR::gar_gce_auth()
gcs_get_object("gs://inhouse-xmap-data/user_data_frames/betalot_experiments-2022-02-21.csv", saveToDisk="/home/ddhillon/projects/beta-av-testing/data/raw/beta-lot-stds-controls-2022-02-21.csv", 
               overwrite = TRUE)
```


```{r}
## import data till 22nd february and pre-process like we usually do 
raw <-
  read_csv(here::here("data", "raw", "beta-lot-stds-controls-2022-02-21.csv")) %>%
  filter(assay %in% c("WFDC2", "CEA", "IL-8", "IL-1 R2", "TNC", "MUC-16", "FLT3L")) %>%
  mutate(batch = str_remove(file_name, "_[0-9]+.csv")) %>%
  filter(!grepl("20211028_Reproducibilit_Panel2_Plate3", file_name)) %>% ## this was a bad batch
  mutate(experiment_date = str_extract(batch, "[0-9]+_")) %>%
  mutate(experiment_date = str_extract(batch, "[0-9]+")) %>%
  mutate(experiment_date = lubridate::as_date(experiment_date, format = "%Y%d%m")) %>%
  mutate(
    month = lubridate::month(experiment_date),
    year = lubridate::year(experiment_date)
  ) %>%
  mutate(bead_fail = ifelse(bead_count < 35, 1, 0)) 

```



## Functions we need 


```{r}
mod_z_calc <- function(x_vec) {
  x_vec_median <- median(x_vec, na.rm = TRUE) 
  x_vec_mad <- mad(x_vec, na.rm = TRUE)
  
  mod_z <- 0.6745*(x_vec - x_vec_median)/x_vec_mad
  return(mod_z)
  
}
### 5PL Recipe 



fit_5pl_v2 <- function(df = NULL){
  nls.multstart::nls_multstart(standard_expected_concentration ~ 
                                          logistic_5pl(d, a, net_mfi = net_mfi, 
                                                       c, b, g), 
                                        data = df, 
                                        iter = 100000, 
                                        modelweights = 1/net_mfi,
                                        start_lower = c(a = -1, b = -1, c = 100, d = -1, 
                                                        g = -1), 
                                        start_upper = c(a = 5, b = 1, c = 200, d = 1, 
                                                        g = 1), 
                                        supp_errors = "Y", 
                                        control = nls.control(maxiter = 1024, 
                                                              minFactor=1e-7, 
                                                              tol=1e-5, 
                                                              printEval=F, 
                                                              warnOnly=F))
}
```

Pick data we want to work with 

1. No bead failures in any of the standards 
2. Take experiments that only have 4 standards in a batch (how our process works) 
3. Exclude standards which have salts mixed in standards (this isn't reflective of our process)   

```{r}
## find all the batches that have 4 replicates for each standard (quads) 
clean_data <- raw %>% 
  filter(bead_fail == 0) %>% ## no bead failures 
  filter(sample_type == "standard") %>% 
  filter(!grepl("Salt", xponent_id)) %>% 
  count(batch, xponent_id, assay) %>% 
  filter(n == 4) %>% 
  distinct(batch) %>% 
  inner_join(raw) %>% 
  ungroup()

clean_std_data <- clean_data %>% 
  filter(sample_type == "standard")
```

## Modeling   



### Model data 

```{r}
mod_data <- clean_std_data %>% 
  group_by(xponent_id, assay, batch) %>% 
  mutate(rep_num = dplyr::row_number(xponent_id)) %>% 
  select(xponent_id, assay, rep_num, median_mfi, 
         standard_expected_concentration, 
         calc_conc, pct_recovery, batch, net_mfi) %>% 
  ungroup()
```

```{r eval=FALSE}
write_csv(mod_data, here::here("data", "processed", "std-curve-replicate-analysis", 
                               "original_mod_data.csv"))
```

### Simulate Missing Data 

```{r}
## these are all the ways we can get 2 replicates 
all_combs <- combn(4, 2) %>% 
  as_tibble(.name_repair = "unique") %>% 
  janitor::clean_names() %>% 
  tidyr::pivot_longer(., cols =c(x1:x6), names_to = "sam", values_to = "rep_num")
```


```{r}
## we create a function that created a dataset with 2 replicates missing from a given standard 
create_missing_data <-
  function(comb = "NULL",
           standard = "NULL",
           data = NULL) {
    rep_comb <- all_combs %>%
      dplyr::filter(sam == comb) %>%
      select(rep_num)
    
    
    tmp_data <- data %>%
      dplyr::filter(xponent_id != standard)
    
    data %>%
      dplyr::filter(xponent_id == standard) %>%
      inner_join(., rep_comb) %>%
      bind_rows(tmp_data)
  }
```

Create sets of missing data 

```{r message=FALSE}
combs <- c("x1", "x2", "x3", "x4", "x5", "x6") 
stds <- c("Standard1", "Standard2", "Standard3", "Standard4", "Standard5", 
          "Standard6")

grid <- tidyr::crossing(combs = combs, stds = stds) %>% 
  mutate(iter = seq(1, nrow(.), by = 1))

missing_data <- grid %>% 
  rowwise(iter) %>% 
  mutate(data = list(create_missing_data(combs, stds, data = mod_data))) %>% 
  unnest(cols = data)

missing_data %>% 
  filter(stds == "Standard1") %>% 
  filter(assay == "CEA") %>% 
  filter(combs == "x2") %>% 
  filter(batch == "20211103_PlatinumStd_Immuno_Panel2_20211103")
```


```{r eval=FALSE}
write_csv(missing_data, here::here("data", "processed", 
                                   "std-curve-replicate-analysis", "missing-sim-data.csv"))
```




## Scipy implementation 

```{r}
library(reticulate)
```

```{r}
Sys.which("python")

use_python(python = "/home/ddhillon/.virtualenvs/pate/bin/python")
```

```{python}
import numpy as np
import pandas as pd

from scipy.optimize.minpack import curve_fit
from joblib import Parallel, delayed
```



```{python}
def curve_fit_coeffs(
    exp_conc_protein: pd.Series, net_mfi_protein: pd.Series) -> np.ndarray:
    """Find a curve fit using the standard concentrations and MFIs from the panel xPonent file.
    Args:
        exp_conc_protein: expected concentrations of the standards
        net_mfi_protein: net MFIs of the standards
    Returns: numpy array of the curve fit coefficients
    """
    min_mfi = net_mfi_protein.min()
    max_mfi = net_mfi_protein.max()
    p0 = [min_mfi, 1, exp_conc_protein.median(), max_mfi, 1]
    xdata = list(exp_conc_protein)
    ydata = list(net_mfi_protein)
    coeffs, _ = curve_fit(
        f=logistic5p,
        xdata=xdata,
        ydata=ydata,
        p0=p0,
        sigma=ydata,
        method="lm",
        maxfev=int(1e5),
        absolute_sigma=True,
    )
    return coeffs

```

```{python}
def logistic5p(x: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """5-parameter logistic equation"""
    return D + (A - D) / ((1 + (x / C) ** B) ** F)
```


```{python}
def inv_logistic5p(y: float, A: float, B: float, C: float, D: float, F: float) -> float:
    """Inverse of the 5-parameter logistic equation"""
    return C * (((((A - D) / (y - D)) ** (1.0 / F)) - 1.0) ** (1.0 / B))
```

```{python}
def parallel_runner(sub_df):
    sub_df = sub_df.loc[~sub_df.isna()["net_mfi"].values]
    estimated_coefficients = curve_fit_coeffs(sub_df["standard_expected_concentration"], sub_df["net_mfi"])
    inferred_concentrations = np.array([inv_logistic5p(net_mfi, *estimated_coefficients) for net_mfi in sub_df["net_mfi"]])
    sub_df["inferred_concentration"] = inferred_concentrations
    return sub_df

```


```{python}
!pwd
```

```{python}
n_cpu = 8
PARALLEL = True
```


```{python}
missing_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/missing-sim-data.csv')
original_df = pd.read_csv('/home/ddhillon/projects/beta-av-testing/data/processed/std-curve-replicate-analysis/original_mod_data.csv')
```
### Fit Clean Data Model  

Fit models to original data to confirm implementation 

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in original_df.groupby(["batch", "assay"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(original_df.groupby(["batch", "assay"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

original_result_df = pd.concat(result_df)
```


```{r}
py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(assay))

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  ggplot(.,aes(xponent_id, pct_recovery_scipy)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(assay))

py$original_result_df %>% 
  mutate(pct_recovery_scipy = 100*(inferred_concentration)/standard_expected_concentration) %>%
  filter(pct_recovery_scipy > 200) %>% 
  filter(assay == "CEA")

```

### Fit Missing Data Models  

Fit models to the missing data we created 

```{python}
if PARALLEL:
    result_df = Parallel(n_cpu)(delayed(parallel_runner)(sub_df) for _, sub_df in missing_df.groupby(["batch", "assay", "combs", "stds"]))
else:
    result_dfs = []
    for i, (_, sub_df) in enumerate(missing_df.groupby(["batch", "assay", "combs", "stds"])):
        result_df = parallel_runner(sub_df)
        result_dfs.append(result_df)

missing_result_df = pd.concat(result_df)
```


```{r}
missing_recov <- py$missing_result_df %>% 
  mutate(pct_recovery_missing = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  group_by(assay, batch, xponent_id, combs, stds) %>% 
  summarise(pct_recovery_missing = mean(pct_recovery_missing))

original_recov <- py$original_result_df %>% 
  mutate(pct_recovery_original = 100*(inferred_concentration)/standard_expected_concentration) %>% 
  group_by(assay, batch, xponent_id) %>% 
  summarise(pct_recovery_original = mean(pct_recovery_original))
```




```{r}
comb_recov <- missing_recov %>% 
  select(c(combs, stds, assay, batch, xponent_id, pct_recovery_missing)) %>% 
  left_join(., original_recov, by = c("assay", "batch", "xponent_id"))
```

```{r}
comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  filter(diff > -200) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_1") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

comb_recov %>% 
  mutate(diff = (pct_recovery_original - pct_recovery_missing)) %>% 
  mutate(perturbed = ifelse(stds == xponent_id, "perturbed_standard", 
                            "not_perturbed"), 
         panel = ifelse(assay %in% c("CEA", "TNC", "WFDC2"), "Panel_2", "Panel_1")) %>% 
  filter(panel == "Panel_2") %>% 
  ggplot(.,aes(xponent_id, diff, color = perturbed)) + 
  geom_boxplot() + 
  facet_grid(cols = vars(stds), rows = vars(assay)) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_color_manual(values = c(freenome_colors[1], freenome_colors[2])) + 
  geom_hline(yintercept = c(-20, 20), linetype = 2)

```










